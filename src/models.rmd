```{r}
model_accuracy <- function(name, model, train, test, characteristic) {
  cat("[DOING]", name, "\n")
  fitted_model <- model |>
    fit(as.formula(paste(characteristic, "~ .")), data = train)

  pred <- fitted_model |>
    predict(test) |>
    pull(.pred_class)

  results <- test |>
    mutate(predictions = pred) |>
    metrics(truth = !!as.name(characteristic), estimate = predictions) |>
    filter(.metric %in% c("accuracy", "kap")) |>
    pivot_wider(names_from = .metric, values_from = .estimate) |>
    select(accuracy, kap)

  return(list(
    "name" = name,
    "model" = fitted_model,
    "metrics" = as.data.frame(results)
  ))
}
```

```{r}
library(ggplot2)
library(tidymodels)
library(GGally)
library(here)
library(caret)
library(vip)
library(discrim)

# Association rules
library(arules)
library(arulesViz)

# Bullshit packages
library(conflicted)
conflicts_prefer(dplyr::setdiff)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
df <- read.csv(here("data", "clean.csv"))
summary(df)
str(df)
```

## Data split

!TO_FORMAL We'll first separate the observations that contain a non numeric `HadHeartAttack` to predict its values in the future

```{r}
heart_attack_target <- df[is.na(df$HadHeartAttack), ] # 2116
df <- df[!is.na(df$HadHeartAttack), ]
df <- df |>
  select(-X) |>
  mutate(HadHeartAttack = as.factor(HadHeartAttack))
```

```{r}
set.seed(123)

had_heart_attack <- df[df$HadHeartAttack == 1, ]

# Undersample to the given target
had_not_heart_attack <- df[df$HadHeartAttack == 0, ] |>
  slice_sample(n = nrow(had_heart_attack))

df <- Reduce(
  function(x, y) merge(x, y, all = TRUE),
  list(had_heart_attack, had_not_heart_attack)
)
```

```{r data_split}
df_split <- initial_split(na.omit(df), prop = 0.70, strata = HadHeartAttack)
train <- training(df_split)
test <- testing(df_split)
```

```{r}
models <- list(
  list(
    "name" = "logistical",
    "model" = logistic_reg(
      mode = "classification",
      engine = "glm",
    )
  ),
  list(
    "name" = "nearest_neighbor",
    "model" = nearest_neighbor(
      mode = "classification",
      engine = "kknn",
      neighbors = 10
    )
  ),
  list(
    "name" = "decision_tree",
    "model" = decision_tree(
      mode = "classification",
      engine = "rpart",
      tree_depth = 20
    )
  ),
  list(
    "name" = "rand_forest",
    "model" = rand_forest(
      mode = "classification"
    ) %>%
      set_engine("ranger", importance = "impurity")
  ),
  list(
    "name" = "bart",
    "model" = bart(
      mode = "classification",
      engine = "dbarts",
      trees = 100
    )
  ),
  list(
    "name" = "mars",
    "model" = mars(
      mode = "classification",
      engine = "earth",
    )
  ),
  list(
    "name" = "nn",
    "model" = mlp(
      mode = "classification",
      engine = "nnet",
      epochs = 100,
      hidden_units = 8,
      learn_rate = 0.01
    )
  ),
  list(
    "name" = "boost_tree",
    "model" = boost_tree(
      mode = "classification",
      engine = "xgboost",
      trees = 100,
      tree_depth = 10,
      min_n = 10,
      loss_reduction = 0.01,
      learn_rate = 0.01
    )
  ),
  list(
    "name" = "lda",
    "model" = discrim_linear(
      mode = "classification",
      engine = "MASS",
    )
  ),
  list(
    "name" = "qda",
    "model" = discrim_quad(
      mode = "classification",
      engine = "MASS",
    )
  )
)

model_results <- map(models, ~ model_accuracy(
  .$name,
  .$model,
  train,
  test,
  "HadHeartAttack"
))
```

o ## PCA

```{r}
hd_num <- mutate_if(df, is.character, as.factor)
hd_num <- mutate_if(hd_num, is.factor, as.numeric)

hd_num <- na.omit(hd_num)

str(hd_num)
pca_mod <- prcomp(hd_num[, -1], scale = TRUE)
cumsum(pca_mod$sdev / sum(pca_mod$sdev))[1:10]

plot(pca_mod$rotation[, 1:2], type = "n")
text(pca_mod$rotation[, 1:2], labels = (colnames(hd_num)[-1]))
```

## Naive Bayes

```{r}
library(e1071)

modelo_nb <- naiveBayes(HadHeartAttack ~ ., data = train)
predicciones <- predict(modelo_nb, newdata = test)
matriz_confusion <- confusionMatrix(predicciones, test$HadHeartAttack)

df_bayes <- data.frame(
  Name = "naive bayes",
  Accuracy = matriz_confusion$overall[[1]],
  Kap = matriz_confusion$overall[[2]]
)
```


## Asoociation rules

```{r}
df2 <- as(data.frame(lapply(df, as.factor)), "transactions")
```

```{r}
# Metodo para sacar los mejores combinaciones de hiperparametros
run_apriori_with_loop <- function(df2, supp_values, conf_values, minlen_values, verbose = FALSE) {
  result_df <- data.frame()

  for (supp_val in supp_values) {
    for (conf_val in conf_values) {
      for (minlen_val in minlen_values) {
        if (verbose) {
          print(paste("[INFO] Doing...", "Support:", supp_val, "Confidence:", conf_val, "MinLength:", minlen_val))
        }
        rules <- apriori(
          df2,
          parameter = list(
            supp = supp_val,
            conf = conf_val,
            minlen = minlen_val
          ),
          appearance = list(rhs = c("HadHeartAttack=1", "HadHeartAttack=0")),
          control = list(verbose = FALSE)
        )

        if (length(rules) == 0) {
          next
        }

        red_rules <- rules
        not_red_rules <- rules[!is.redundant(rules)]
        rules_conf <- sort(not_red_rules, by = "lift", decreasing = TRUE)

        result_df <- rbind(
          result_df,
          data.frame(
            Support = supp_val,
            Confidence = conf_val,
            MinLength = minlen_val,
            NumRedundant_Rules = length(red_rules),
            NumNon_Redundant_Rules = length(rules_conf)
          )
        )
      }
    }
  }

  return(result_df)
}
```

```{r}
supp_values <- c(0.3, 0.4)
conf_values <- c(0.6, 0.7, 0.8, 0.9)
minlen_values <- c(2, 3)

result_rules <- run_apriori_with_loop(
  df2,
  supp_values,
  conf_values,
  minlen_values,
  verbose = TRUE
)

result_rules
```

After extracting the best hyper-parameters and searching for association rules, we can observe that we haven't managed to extract enough rules with sufficient confidence. Out of approximately 2000 non-redundant rules, only 8 of them have a confidence greater than 0.7. Therefore, they are not a reliable indicator either to determine if a patient suffered a heart attack or to identify the risk factors.

```{r}
rules <- apriori(
  df2,
  parameter = list(
    supp = 0.3,
    conf = 0.6,
    minlen = 3
  ),
  appearance = list(rhs = c("HadHeartAttack=1", "HadHeartAttack=0")),
  control = list(verbose = FALSE)
)
```

```{r}
plot(rules, method = "two-key plot")
plot(rules, method = "graph")
```

## Model results

Once we have all the models, we can compare them to see which one is the best:

```{r}
df_res <- data.frame(row.names = c("Name", "Accuracy", "Kap"))
for (result in model_results) {
  df_res <- rbind(
    df_res,
    data.frame(
      Name = result$name,
      Accuracy = result$metrics$accuracy,
      Kap = result$metrics$kap
    )
  )
}

df_res <- rbind(df_res, df_bayes)
df_res <- df_res[order(-df_res$Accuracy, decreasing = FALSE), ]
df_res
```

```{r}
library(gridExtra)
grid.arrange(
  ggplot(df_res, aes(x = Name, y = Kap, fill = Name)) +
    geom_bar(stat = "identity") +
    labs(title = "Kappa por Modelo", x = "Modelo", y = "Kappa") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)),
  ggplot(df_res, aes(x = Name, y = Accuracy, fill = Name)) +
    geom_bar(stat = "identity") +
    labs(title = "Accuracy por Modelo", x = "Modelo", y = "Accuracy") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)),
  ncol = 2
)
```

As we can see, the models have a very similar accuracy and kappa values, making it difficult to choose one over the other. However, we will choose the models that the result of the model can be explained, such as the decision tree, random forest, xgboost, Naive Bayes, logistic regression, multiLayer perceptron, LDA and QDA.

## Model explanation

```{r}
library(rpart.plot)
library(rattle)
library(vip)
```

### Decision tree

```{r}
rpart.plot(model_results[[3]]$model$fit, type = 4, extra = 1, roundint = FALSE)
```

Upon splitting the data using the decision tree, we can appreciate that the most important features are `HadAngina`, `ChestScan`, `AgeCategory`, `HadStroke`, and `GeneralHealth`.

### Random forest

```{r}
tree <- model_results[[4]]$model$fit$variable.importance
importance <- (tree / sum(tree)) * 100

df_importancia <- data.frame(importance)

ggplot(df_importancia, aes(x = rownames(df_importancia), y = importance, fill = rownames(df_importancia))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Variable importance", x = "Variable", y = "Porcentaje") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
```

As we can see, the most important variable is `HadAngina` with a big difference from the rest of the variables, nearly a 25% of importance.

### XGBoost

```{r}
vip(model_results[[8]]$model$fit)
```

In this model we can observe that the dominant variable is `hadAngina` with a value near to 60% meanwhile the the second characteristic, `ChestScan`, is less than a 20%.

### Naive Bayes

```{r}
freq <- t(data.frame(modelo_nb$tables))
colnames(freq)[colnames(freq) == "0"] <- "No"
colnames(freq)[colnames(freq) == "1"] <- "Yes"
freq <- data.frame(freq)

ggplot(freq, aes(x = rownames(freq), y = Yes, fill = rownames(freq))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Naive Bayes Variable importance for Yes", x = "Variable", y = "Porcentaje") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")
```

```{r}
ggplot(freq, aes(x = rownames(freq), y = No, fill = rownames(freq))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Naive Bayes Variable importance for No", x = "Variable", y = "Porcentaje") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")
```

With this models we have a little of variance between the importance of the variables depending of the predicted value. In botsh cases the most dominant characteristic is State being the 'yes' value more significant, having near a 30% of the importance of the variables. However variables like Age `AgeCategory` when the `HadHeartAttack` value is yes the `AgeCategory` has a 5% importance while in the other case the importance of the variables is less than 5%.

### Linear Regression

```{r}
library(broom)
coefs <- tidy(model_results[[1]]$model, conf.int = TRUE)
```

```{r}
ggplot(coefs, aes(x = reorder(term, estimate), y = estimate, fill = estimate > 0)) +
  geom_bar(stat = "identity", position = "identity", color = "black") +
  coord_flip() +
  labs(
    title = "Importancia de Coeficientes",
    x = "Coeficientes",
    y = "Valor"
  ) +
  scale_fill_manual(values = c("red", "green")) +
  theme_minimal()
```

Observing this plot the most important variables are `HadAngina` with an importance near to 50% and `HadStroke` with a 20%

### MultiLayer Perceptron

```{r}
coefs <- coef(model_results[[7]]$model$fit)
coefs_df <- data.frame(term = names(coefs), estimate = coefs)

ggplot(coefs_df, aes(x = reorder(term, estimate), y = estimate, fill = estimate > 0)) +
  geom_bar(stat = "identity", position = "identity", color = "black") +
  coord_flip() +
  labs(
    title = "Importancia de Pesos de Conexiones en MLP",
    x = "Conexiones",
    y = "Peso",
    subtitle = "Coeficientes de Pesos de Conexiones en MLP"
  ) +
  scale_fill_manual(values = c("red", "green")) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8, hjust = 0.5), axis.text.x = element_blank())
```

```{r}
top_coefs <- coefs_df %>%
  arrange(desc(abs(estimate))) %>%
  head(10)

ggplot(top_coefs, aes(x = reorder(term, estimate), y = estimate, fill = estimate > 0)) +
  geom_bar(stat = "identity", position = "identity", color = "black") +
  labs(
    title = "Top 10 Coeficientes de Pesos de Conexiones en MLP",
    x = "Conexiones",
    y = "Peso",
    subtitle = "Los 10 coeficientes más importantes de Pesos de Conexiones en MLP"
  ) +
  scale_fill_manual(values = c("red", "green")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), axis.title.x = element_blank())
```

With the full conecctions of characteristics was difficult to visualize correctcly the most significant characteristics so we filter them to take the 10 most importants, the problem with this model is that is difficult to interpret the result so this model will not help us to answer our question.

### LDA

```{r}
coefs_lda <- model_results[[9]]$model$fit

coefs_df <- data.frame(
  term = rownames(coefs_lda$scaling),
  estimate = coefs_lda$scaling[, 1] # Tomar solo la primera columna (pueden haber más si hay más clases)
)

# Crear un gráfico de barras para visualizar los coeficientes
ggplot(coefs_df, aes(x = reorder(term, estimate), y = estimate, fill = estimate > 0)) +
  geom_bar(stat = "identity", position = "identity", color = "black") +
  labs(
    title = "Coeficientes de Análisis Discriminante Lineal (LDA)",
    x = "Predictores",
    y = "Coeficiente",
    subtitle = "Importancia de los coeficientes en el modelo lda"
  ) +
  scale_fill_manual(values = c("red", "green")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
    axis.title.x = element_blank()
  )
```

```{r}
df_means_long <- data.frame(t(data.frame(coefs_lda$means)))

ggplot(df_means_long, aes(x = rownames(df_means_long), y = X0, fill = rownames(df_means_long))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Valores Medios de Grupo por Variable", x = "Variable", y = "Valor Medio") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")

ggplot(df_means_long, aes(x = rownames(df_means_long), y = X1, fill = rownames(df_means_long))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Valores Medios de Grupo por Variable", x = "Variable", y = "Valor Medio") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")
```

The result of this model tells us that the dominant variables are `HadAngina` with a coefficient bigger than 1.5, then we have `HadStroke`, `ChestScan` and `Gender` with coefficients near 0.5.

### QDA

```{r}
coefs_lda <- model_results[[10]]$model$fit

coefs_df <- data.frame(
  term = rownames(coefs_lda$scaling),
  estimate = coefs_lda$scaling[, 1]
)

# Crear un gráfico de barras para visualizar los coeficientes
ggplot(coefs_df, aes(x = reorder(term, estimate), y = estimate, fill = estimate > 0)) +
  geom_bar(stat = "identity", position = "identity", color = "black") +
  labs(
    title = "Coeficientes de Análisis Discriminante Lineal (LDA)",
    x = "Predictores",
    y = "Coeficiente",
    subtitle = "Importancia de los coeficientes en el modelo lda"
  ) +
  scale_fill_manual(values = c("red", "green")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), axis.title.x = element_blank())
```

```{r}
df_means_long <- data.frame(t(data.frame(coefs_lda$means)))

ggplot(df_means_long, aes(x = rownames(df_means_long), y = X0, fill = rownames(df_means_long))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Valores Medios de Grupo por Variable", x = "Variable", y = "Valor Medio") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")

ggplot(df_means_long, aes(x = rownames(df_means_long), y = X1, fill = rownames(df_means_long))) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Valores Medios de Grupo por Variable", x = "Variable", y = "Valor Medio") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")
```

## Making predictions for `HadHeartAttack` missing values

```{r}
results <- data.frame(
  "name" = character(),
  "result" = character()
)

for (result in model_results) {
  print(paste("[INFO] Doing...", result$name))
  pred <- result$model |>
    predict(heart_attack_target) |>
    pull(.pred_class)

  results <- rbind(
    results,
    data.frame(
      "name" = result$name,
      "result" = pred
    )
  )
}
```

```{r}
frequency_results <- results %>%
  group_by(name, result) %>%
  summarise(frequency = n()) %>%
  mutate(percentage = (frequency / sum(frequency)) * 100) %>%
  mutate(result = ifelse(result == 0, "No", "Yes"))

ggplot(frequency_results, aes(x = name, y = percentage, fill = result)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Porcentaje de Resultados por Modelo y Categoría",
    x = "Modelo", y = "Porcentaje"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r doktor}
doktor <- function(observation, models) {
  predictions <- map( # nolint
    models, ~ .$model |>
      predict(observation) |>
      pull(.pred_class)
  ) |>
    unlist() |>
    table() |>
    prop.table()

  return(predictions)
}

doktor(heart_attack_target[1, ], model_results)
```

As such, using the `doktor` function any user or programmer could predict given the transformation pipeline expressed on this paper with a `softmax`-like result.
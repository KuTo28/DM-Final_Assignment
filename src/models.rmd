```{r}
model_accuracy <- function(name, model, train, test, characteristic) {
  cat("[DOING]", name, "\n")
  fitted_model <- model |>
    fit(as.formula(paste(characteristic, "~ .")), data = train) # nolint

  pred <- fitted_model |>
    predict(test) |>
    pull(.pred_class) # nolint

  results <- test |>
    mutate(predictions = pred) |> # nolint
    metrics(truth = !!as.name(characteristic), estimate = predictions) |> # nolint
    filter(.metric %in% c("accuracy", "kap")) |> # nolint
    pivot_wider(names_from = .metric, values_from = .estimate) |> # nolint
    select(accuracy, kap) # nolint

  return(list(
    "name" = name,
    "model" = fitted_model,
    "metrics" = as.data.frame(results)
  ))
}
```

```{r}
library(tidymodels)
library(GGally)
library(here)
library(caret)
library(vip)
library(discrim)

# Association rules
library(arules)
library(arulesViz)

# Bullshit packages
library(conflicted)
conflicts_prefer(dplyr::setdiff)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
df <- read.csv(here("data", "clean.csv"))
summary(df)
str(df)
```

## Data split

!TO_FORMAL We'll first separate the observations that contain a non numeric `HadHeartAttack` to predict its values in the future

```{r}
heart_attack_target <- df[is.na(df$HadHeartAttack), ] # 2116
df <- df[!is.na(df$HadHeartAttack), ]
df <- df |>
  select(-X) |>
  mutate(HadHeartAttack = as.factor(HadHeartAttack))
```

```{r}
set.seed(123)

had_heart_attack <- df[df$HadHeartAttack == 1, ]

# Undersample to the given target
had_not_heart_attack <- df[df$HadHeartAttack == 0, ] |>
  slice_sample(n = nrow(had_heart_attack))

df <- Reduce(
  function(x, y) merge(x, y, all = TRUE),
  list(had_heart_attack, had_not_heart_attack)
)
```

```{r data_split}
df_split <- initial_split(na.omit(df), prop = 0.70, strata = HadHeartAttack)
train <- training(df_split)
test <- testing(df_split)
```

```{r}
models <- list(
  list(
    "name" = "logistical",
    "model" = logistic_reg(
      mode = "classification",
      engine = "glm",
    )
  ),
  list(
    "name" = "nearest_neighbor",
    "model" = nearest_neighbor(
      mode = "classification",
      engine = "kknn",
      neighbors = 10
    )
  ),
  list(
    "name" = "svm_linear",
    "model" = svm_linear(
      mode = "classification",
      engine = "kernlab",
    )
  ),
  list(
    "name" = "svm_rbf",
    "model" = svm_rbf(
      mode = "classification",
      engine = "kernlab",
    )
  ),
  list(
    "name" = "decision_tree",
    "model" = decision_tree(
      mode = "classification",
      engine = "rpart",
      tree_depth = 20
    )
  ),
  list(
    "name" = "rand_forest",
    "model" = rand_forest(
      mode = "classification"
    ) %>%
      set_engine("ranger", importance = "impurity")
  ),
  list(
    "name" = "bart",
    "model" = bart(
      mode = "classification",
      engine = "dbarts",
      trees = 100
    )
  ),
  list(
    "name" = "mars",
    "model" = mars(
      mode = "classification",
      engine = "earth",
    )
  ),
  list(
    "name" = "nn",
    "model" = mlp(
      mode = "classification",
      engine = "nnet",
      epochs = 100,
      hidden_units = 8,
      learn_rate = 0.01
    )
  ),
  list(
    "name" = "boost_tree",
    "model" = boost_tree(
      mode = "classification",
      engine = "xgboost",
      trees = 100,
      tree_depth = 10,
      min_n = 10,
      loss_reduction = 0.01,
      learn_rate = 0.01
    )
  ),
  list(
    "name" = "lda",
    "model" = discrim_linear(
      mode = "classification",
      engine = "MASS",
    )
  ),
  list(
    "name" = "qda",
    "model" = discrim_quad(
      mode = "classification",
      engine = "MASS",
    )
  )
)

model_results <- map(models, ~ model_accuracy(
  .$name,
  .$model,
  train,
  test,
  "HadHeartAttack"
))
```

```{r model feature importance}
# logistical, decision_tree, rand_forest, nn
# model_results[[2]]$model$fit %>%
#  vip(num_features = 20)
```

```{r}
f_importances <- function(coef, names) {
  imp <- coef
  ordering <- order(imp)
  imp <- imp[ordering]
  names <- names[ordering]
  barplot(imp, names.arg = names, horiz = TRUE, col = "lightblue", main = "Feature Importances")
}

svm_fit <- coef(model_results[[2]]$model$fit)

svm_fit
# Visualizar la importancia de características
f_importances(svm_fit, colnames(svm_fit$workflow$steps$svm_linear$fit$fit$call$x))
```

## PCA

```{r}
hd_num <- mutate_if(df, is.character, as.factor)
hd_num <- mutate_if(hd_num, is.factor, as.numeric)

hd_num <- na.omit(hd_num)

str(hd_num)
pca_mod <- prcomp(hd_num[, -1], scale = TRUE)
cumsum(pca_mod$sdev / sum(pca_mod$sdev))[1:10]

plot(pca_mod$rotation[, 1:2], type = "n")
text(pca_mod$rotation[, 1:2], labels = (colnames(hd_num)[-1]))
```

## Naive Bayes

```{r}
library(e1071)

modelo_nb <- naiveBayes(HadHeartAttack ~ ., data = train)

predicciones <- predict(modelo_nb, newdata = test)

matriz_confusion <- confusionMatrix(predicciones, test$HadHeartAttack)

print(matriz_confusion)

# Obtener las tablas de probabilidades
tabla_frecuencias <- table(predicciones, test$HadHeartAttack)

# Imprime la tabla de frecuencias
print(tabla_frecuencias)

# Calcula la proporción de predicciones correctas para cada nivel de la variable de respuesta
proporcion_correctas <- diag(prop.table(tabla_frecuencias, margin = 2))

# Imprime la proporción de predicciones correctas para cada nivel de la variable de respuesta
print(proporcion_correctas)
```

## Asoociation rules

```
{r}
df2 <- as(data.frame(lapply(df, as.factor)), "transactions")
```

```
{r}
# Metodo para sacar los mejores combinaciones de hiperparametros
run_apriori_with_loop <- function(df2, supp_values, conf_values, minlen_values, verbose = FALSE) {
  result_df <- data.frame()

  for (supp_val in supp_values) {
    for (conf_val in conf_values) {
      for (minlen_val in minlen_values) {
        if (verbose) {
          print(paste("[INFO] Doinng...", "Support:", supp_val, "Confidence:", conf_val, "MinLength:", minlen_val))
        }
        rules <- apriori(
          df2,
          parameter = list(
            supp = supp_val,
            conf = conf_val,
            minlen = minlen_val
          ),
          appearance = list(rhs = c("HadHeartAttack=1", "HadHeartAttack=0")),
          control = list(verbose = FALSE)
        )
        if (length(rules) == 0) {
          next
        }
        red_rules <- rules
        not_red_rules <- rules[!is.redundant(rules)]
        rules_conf <- sort(not_red_rules, by = "lift", decreasing = TRUE)

        result_df <- rbind(
          result_df,
          data.frame(
            Support = supp_val,
            Confidence = conf_val,
            MinLength = minlen_val,
            NumRedundant_Rules = length(red_rules),
            NumNon_Redundant_Rules = length(rules_conf)
          )
        )
      }
    }
  }

  return(result_df)
}
```

```
{r}
supp_values <- c(0.3, 0.4)
conf_values <- c(0.6, 0.7, 0.8, 0.9)
minlen_values <- c(2, 3)

result_rules <- run_apriori_with_loop(
  df2,
  supp_values,
  conf_values,
  minlen_values,
  verbose = TRUE
)
result_rules
```

```
{r}
rules <- apriori(
  df2,
  parameter = list(
    supp = 0.3,
    conf = 0.6,
    minlen = 3
  ),
  appearance = list(rhs = c("HadHeartAttack=1", "HadHeartAttack=0")),
  control = list(verbose = FALSE)
)
```

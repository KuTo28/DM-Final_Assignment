## Introduction

The exploration of health-related datasets has emerged as an important endeavor in advancing public health research. This paper aims to explore the Behavioral Risk Factor Surveillance System (BRFSS) dataset, which conducts annual telephone surveys to collect data on the health status of U.S. to detect possible patters that allow us to detect if a patient is at risk to have a heart attack. This research bears significance in contributing to early detection and risk assessment, influencing targeted healthcare interventions.

The original dataset consist of 445132 observations with 40 variables of categorical or numerical nature:

- `State`: State of residence [chr]
- `Gender`: Gender of Resident [chr: Female, Male]
- `GeneralHealth`: Would you say that in general your health is: [chr: Excellent, Very good, Good, Fair, Poor]
- `PhysicalHealthDays`: Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? [num: 0, 1, ..., 30]
- `MentalHealthDays`: Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good? [num: 0, 1, ..., 30]
- `LastCheckupTime`: About how long has it been since you last visited a doctor for a routine checkup? [chr: Within the past year (less than 12 months ago), Within past 2 years (1 year but less than 2 years ago), Within past 5 years (2 years but less than 5 years ago)]
- `PhysicalActivities`: During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise? [chr: No, Yes]
- `SleepHours`: On average, how many hours of sleep do you get in a 24-hour period? [num: 0, 1, ..., 24]
- `RemovedTeeth`: Not including teeth lost for injury or orthodontics, how many of your permanent teeth have been removed because of tooth decay or gum disease? [chr: All, 6 or more (but not all), 1 to 5, None of them]
- `HadHeartAttack`: (Ever been told) you had a heart attack, also called a myocardial infarction? [chr: No, Yes]
- `HadAngina`: (Ever been told) you had angina or coronary heart disease? [chr: No, Yes]
- `HadStroke`: (Ever been told) you had a stroke? [chr: No, Yes]
- `HadAsthma`: (Ever been told) you had asthma? [chr: No, Yes]
- `HadSkinCancer`: (Ever been told) you had skin cancer that is not melanoma? [chr: No, Yes]
- `HadCOPD`: (Ever been told) you had C.O.P.D. (chronic obstructive pulmonary disease), emphysema or chronic bronchitis? [chr: No, Yes]
- `HadDepressiveDisorder`: (Ever been told) you had a depressive disorder (including depression, major depression, dysthymia, or minor depression)? [chr: No, Yes]
- `HadKidneyDisease`: Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease? [chr: No, Yes]
- `HadArthritis`: (Ever been told) you had some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia? (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel syndrome, tarsal tunnel syndrome; joint infection, etc.) [chr: No, Yes]
- `HadDiabetes`: (Ever been told) you had diabetes? [chr: No, No only pre-diabetes or borderline diabetes, Yes, Yes but only during pregnancy (female)]
- `DeafOrHardOfHearing`: Are you deaf or do you have serious difficulty hearing? [chr: No, Yes]
- `BlindOrVisionDifficulty`: Are you blind or do you have serious difficulty seeing, even when wearing glasses? [chr: No, Yes]
- `DifficultyConcentrating`: Because of a physical, mental, or emotional condition, do you have serious difficulty concentrating, remembering, or making decisions? [chr: No, Yes]
- `DifficultyDressingBathing`: Do you have difficulty dressing or bathing? [chr: No, Yes]
- `DifficultyErrands`: Because of a physical, mental, or emotional condition, do you have difficulty doing errands alone such as visiting a doctorÂ´s office or shopping? [chr: No, Yes]
- `SmokerStatus`: Four-level smoker status [chr: Everyday smoker, Someday smoker, Former smoker, Never smoked]
- `ECigarretteUsage`: Would you say you have never used e-cigarettes or other electronic vaping products in your entire life or now use them every day, use them some days, or used them in the past but do not currently use them at all? [chr: Use them every day, Use them some days, Not at all (right now), Never used e-cigarettes in my entire life]
- `ChestScan`: Have you ever had a CT or CAT scan of your chest area? [chr: No, Yes]
- `RaceEthnicityCategory`: Five-level race/ethnicity category [chr: White only Non-Hispanic, Black only Non-Hispanic, Hispanic, Multiracial Non-Hispanic, Other race only Non-Hispanic]
- `AgeCategory`: Fourteen-level age category [chr: 50 to 54, 55 to 59, 60 to 64, 65 to 69, 70 o 74, 75 to 79, 80 or older]
- `HeightInMeters`: Reported height in meters [num]
- `WeightInKilograms`: Reported weight in kilograms [num]
- `BMI`: Body Mass Index [num]
- `AlcoholDrinkers`: Adults who reported having had at least one drink of alcohol in the past 30 days. [chr: No, Yes]
- `HIVTesting`: Adults who have ever been tested for HIV [chr: No, Yes]
- `FluVaxLast12`: During the past 12 months, have you had either flu vaccine that was sprayed in your nose or flu shot injected into your arm? [chr: No, Yes]
- `PneumoVaxEver`: Have you ever had a pneumonia shot also known as a pneumococcal vaccine? [chr: No, Yes]
- `TetanusLast10Tday`: Have you received a tetanus shot in the past 10 years? Was this Tdap, the tetanus shot that also has pertussis or whooping cough vaccine?[chr: Yes received Tdap, Yes received tetanus shot but not sure what type,Yes received tetanus shot but not Tdap, No did not receive any tetanus shot in the past 10 years]
- `HighRiskLastYear`: You have injected any drug other than those prescribed for you in the past year. You have been treated for a sexually transmitted disease or STD in the past year. You have given or received money or drugs in exchange for sex in the past year. [chr: No, Yes]
- `CovidPos`: Has a doctor, nurse, or other health professional ever told you that you tested positive for COVID 19? [chr: No, Tested positive using home test, Yes]

Formally, we aim to address whether a patient will experience a heart attack by employing an array of predictive models and selecting the most frequent result. Furthermore, we'll provide the features that most contribute to this prediction analyzing some of the used models.

For it, we'll need to clean the data set to best fit our predictive models by:

- Reducing the amount of features
- Apply feature engineering
- Merge variables that present high colinearity
- Replace missing data

## Helpers

Here we define a set of functions that might prove useful to increase the readability of the code

This function will be used to train and evaluate different models.

```{r predictive_model helper}
model_accuracy <- function(model, train, test, characteristic) {
  fitted_model <- model |>
    fit(as.formula(paste(characteristic, "~ .")), data = train) # nolint

  pred <- fitted_model |>
    predict(test) |>
    pull(.pred_class) # nolint

  results <- test |>
    mutate(predictions = pred) |>
    metrics(truth = !!as.name(characteristic), estimate = predictions) |> # nolint
    filter(.metric %in% c("accuracy", "kap")) |> # nolint
    pivot_wider(names_from = .metric, values_from = .estimate) |> # nolint
    select(accuracy, kap) # nolint

  return(list("model" = fitted_model, "metrics" = as.data.frame(results)))
}
```

Function used to obtain the statistics of a model.

```{r stats model helper}
model_statistics <- function(num_samples, target_variable, dataframe) {
  df2 <- dataframe

  # Sample non-NA indices
  non_na_indices <- which(!is.na(df2[[target_variable]]))
  sample_indices <- sample(non_na_indices, num_samples)

  df2[[target_variable]][sample_indices] <- NA

  # Build model
  model_formula <- formula(paste(target_variable, "~ ."))
  weight_model <- rpart(model_formula, data = df2, method = "anova")

  # Predict for NA values
  df2[[target_variable]][is.na(df2[[target_variable]])] <-
    predict(weight_model, newdata = df2[is.na(df2[[target_variable]]), ])

  # Actual values for comparison
  actual_values <- dataframe[[target_variable]][sample_indices]

  # Calculate accuracy and standard deviation
  accuracy <- 1 - sum((df2[[target_variable]][sample_indices] - actual_values)^2) / sum((actual_values - mean(actual_values))^2)
  std_deviation <- sd(df2[[target_variable]][sample_indices] - actual_values)

  # Return results
  return(tibble(accuracy = accuracy, std_deviation = std_deviation))
}
```

Function to plot pie plots

```{r plot pie helper}
plot_pie_freq <- function(variable, plot_name) {
  data <- data.frame(table(variable))

  color_palette <- pals::stepped3(n = length(unique(variable)))
  ggplot(data, aes(x = "", y = Freq, fill = variable)) +
    geom_bar(stat = "identity", width = 1) +
    geom_text(
      aes(x = 1.6, label = sprintf("%.1f%%", Freq / sum(Freq) * 100)),
      position = position_stack(vjust = 0.5),
      size = 4
    ) +
    coord_polar(theta = "y") +
    labs(title = plot_name) +
    scale_fill_manual(values = color_palette) +
    theme_void() +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      plot.title = element_text(hjust = 0.5)
    )
}
```

Functions to make dendrograms.

```{r hclust helper}
perform_hclust <- function(data, method, num_clusters) {
  hc <- hclust(dist(data), method = method)
  hc <- cutree(hc, k = num_clusters)
  return(list(
    method = method,
    num_clusters = num_clusters,
    silhouette = sum(data.frame(silhouette(hc, dist(data)))$sil_width)
  ))
}

plot_hclust <- function(data, method, num_clusters, plot_name) {
  hc <- as.dendrogram(hclust(dist(data), method = method))
  hc <- color_branches(hc, k = num_clusters)

  fviz_dend(
    hc,
    main = plot_name,
    show_labels = FALSE
  ) +
    theme( # nolint
      plot.title = element_text(size = 14, hjust = 0.5), # nolint
    )
}

make_hclust <- function(variable, plot_name) {
  parameters <- expand.grid(
    method = c(
      "single", "complete", "average", "mcquitty", "median", "centroid"
    )
  )

  hclust_metrics <- bind_rows(
    pmap(parameters, ~ perform_hclust(variable, ..1, num_clusters = 3))
  )

  best_result <- hclust_metrics[which.max(hclust_metrics$silhouette), ]


  plot_hclust(variable, best_result$method, best_result$num_clusters, plot_name)

  return(cutree(
    hclust(dist(variable), method = best_result$method),
    k = best_result$num_clusters
  ))
}
```

## Setup

```{r libraries, message=FALSE}
library(dplyr)
library(ggplot2)
library(here)
library(purrr)
library(rpart)
library(statebins)
library(tidygeocoder)
library(leaflet)
library(gridExtra)
library(GGally)
library(stringr)
library(pals)
library(cluster)
library(factoextra)
library(tidymodels)
library(caret)
library(glue)
library(dbscan) # Density based cluster
library(patchwork) # Plot composer
library(dendextend) # Color dedongram by cluster
library(arules)
```

```{r df load}
df <- read.csv(here("data", "heart_data.csv"))

df[df == ""] <- NA
df <- df |> mutate(across(where(is.character), as.factor))
```

```{r df summary}
str(df)
summary(df)
```

Exploring our target variable:

```{r}
plot_pie_freq(df$HadHeartAttack, "Heart Attack distribution")
```

We can observe that it's quite unbalanced, only `5.7%` of observations present having a heart attack. To solve this class unbalance, we'll employ data [undersampling](https://en.wikipedia.org/wiki/Undersampling) to reduce the amount of observations to the class exhibiting the least frequency; in this case, patients having a heart attack

## Raw Data Analysis

### Non-numeric values

One of the best practices is to perform data analysis on the dataset to understand what we have and how it is structured. This will help us determine if there is a need to correct or eliminate values to ensure that our study and tests are accurate, without being affected by data inconsistencies. So, let's start by visualizing if we have any missing values and examining their distribution.

```{r plot NA_col pie}
pie_data <- data.frame(
  category = c("NA", "Non-NA"),
  count = c(sum(is.na(df)), sum(!is.na(df)))
) |>
  mutate(
    percentage = round(count / sum(count) * 100, 2)
  )

color_palette <- pals::stepped3(n = length(unique(pie_data)))
ggplot(pie_data, aes(x = "", y = count, fill = category)) +
  geom_bar(stat = "identity", width = 1) +
  geom_text(
    aes(x = 1.6, label = sprintf("%.1f%%", count / sum(count) * 100)),
    position = position_stack(vjust = 0.5),
    size = 4
  ) +
  coord_polar(theta = "y") +
  labs(title = "NA vs Non-NA Counts") +
  scale_fill_manual(values = color_palette) +
  theme_void() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

As we can see there is a `5.1%` of NA's and `94.4%` of Non-NA out of `17.805.280` values. If we want a deeper examination we can look the amount of NA's found for each column.

```{r plot NA_col}
na_df <- tibble(
  column = names(df),
  na_count = colSums(is.na(df))
) |>
  filter(na_count > 0) |>
  arrange(desc(na_count))

ggplot(
  na_df,
  aes(
    x = column,
    y = na_count,
    label = scales::percent(na_count / sum(na_count), accuracy = 0.01)
  )
) +
  geom_bar(
    stat = "identity",
    fill = "skyblue",
    position = "dodge"
  ) +
  geom_text(
    aes(label = scales::percent(na_count / sum(na_count), accuracy = 0.01)),
    position = position_dodge(width = 1),
    vjust = 0.4,
    hjust = -0.3,
    size = 3
  ) +
  labs(title = "NA Counts for Each Column") +
  theme_minimal() +
  theme(
    axis.text.y = element_text(hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_flip()
```

In this table we can see the percentages of NA's for each column of the dataset. The column with the highest amount of NA's is `TetanusLast10TDap` `(9.1%)` and the one with the least of amount are `HadDiabetes` and `PhysicalActivities` `(0.12%)`. Because of the different types of variables, we are going to take care of the NA's in different ways.

As the amount of NA values is considerable, we'll study the distribution of them per rows to further identify the severity of these missing values:

```{r df NA_row}
na_group_threshold <- 5

na_count <- df |>
  is.na() |>
  rowSums() |>
  table() |>
  as.data.frame() |>
  mutate(across(is.factor, as.numeric)) |>
  rename(num_na = Var1) |>
  mutate(num_na = num_na - 1)

na_count_unbounded <- na_count

na_count <- na_count |>
  filter(num_na <= na_group_threshold) |>
  mutate(num_na = as.character(num_na)) |>
  bind_rows(
    data.frame(
      num_na = paste0(na_group_threshold, "+"),
      Freq = sum(na_count$Freq[as.numeric(na_count$num_na) > na_group_threshold])
    )
  )
```

```{r plot NA_row bar}
ggplot(
  na_count_unbounded,
  aes(
    x = num_na,
    y = Freq,
    label = scales::percent(Freq / sum(Freq), accuracy = 0.0001)
  )
) +
  geom_bar(
    stat = "identity",
    fill = "skyblue",
    position = "dodge"
  ) +
  geom_text(
    aes(label = scales::percent(Freq / sum(Freq), accuracy = 0.0001)),
    position = position_dodge(width = 1), # Adjust width as needed
    vjust = 0.4, # Adjust vertical position of labels
    hjust = -0.3,
    size = 3 # Adjust text size
  ) +
  labs(title = "NA Distribution per Rows") +
  theme_minimal() +
  theme(
    axis.text.y = element_text(hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_flip() +
  scale_x_continuous(
    breaks = na_count_unbounded$num_na,
    labels = na_count_unbounded$num_na
  )
```

```{r plot NA_row pie}
color_palette <- pals::stepped3(n = length(unique(na_count$num_na)))
ggplot(na_count, aes(x = "", y = Freq, fill = num_na, label = sprintf("%.1f%%", Freq / sum(Freq) * 100))) +
  geom_bar(stat = "identity", width = 1) +
  geom_text(
    aes(x = 1.6, label = sprintf("%.1f%%", Freq / sum(Freq) * 100)),
    position = position_stack(vjust = 0.5),
    size = 4
  ) + # Add labels
  coord_polar(theta = "y") +
  labs(title = "Distribution of NA Counts per Row") +
  scale_fill_manual(values = color_palette) + # Use the chosen color palette
  theme_void() +
  theme(
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

As shown by the pie chart above, there are `10.7%` of observations that contain more than 10 NA. These observations could be deemed as impractical for our data analysis due to the extensive data corruption. As such, these could be systematically removed in subsequent stages of the analysis. The decision to remove them instead of manage them is related on the acknowledgment that these observations could mislead the data analysis.

## Data cleanup

### Removal of "corrupted" data

Since we have several rows with many missing values deemed as _corrupt observations_, we'll proceed to remove them. If the number of missing values in a row exceeds **5**, the corresponding row will be deleted.

```{r NA remove}
df <- df[rowSums(is.na(df)) <= 5, ]
```

### Variable recodifications

We continue the data processing by categorizing the different variables, as it will assist us in interpreting the data. For example, grouping ages will make it easier to identify patterns. This will help in the future to reduce the complexity of linear models and trees.

#### Age

We'll group the ages in the following categories, based on our own criteria:

- Very young: [0,25)
- Young: [25,35)
- Mature: [35,50)
- Senior: [50,65)
- Old: [65,80)
- Very old: [80,inf)

```{r age_bin}
df <- df |>
  mutate(
    AgeCategory = case_when(
      AgeCategory %in% c("Age 18 to 24") ~ "very young",
      AgeCategory %in% c("Age 25 to 29", "Age 30 to 34") ~ "young",
      AgeCategory %in% c("Age 35 to 39", "Age 40 to 44", "Age 45 to 49") ~ "mature",
      AgeCategory %in% c("Age 50 to 54", "Age 55 to 59", "Age 60 to 64") ~ "senior",
      AgeCategory %in% c("Age 65 to 69", "Age 70 to 74", "Age 75 to 79") ~ "old",
      AgeCategory %in% c("Age 80 or older") ~ "very old",
      is.na(AgeCategory) ~ "not known",
      TRUE ~ AgeCategory
    )
  ) |>
  mutate(AgeCategory = factor(
    AgeCategory,
    levels = c("very young", "young", "mature", "senior", "old", "very old", "not known")
  ))

df |> count(AgeCategory)
```

Finally, we can see the distribution of age between the data of the dataset, we can see that there is approximately a `63%` of people older than 50.

```{r plot age_dist pie}
plot_pie_freq(df$AgeCategory, "Age distribution")
```

#### Tetanus

```{r tetanus_map}
df <- df |>
  mutate(
    TetanusVaccinated = ifelse(
      str_detect(TetanusLast10Tdap, "^Yes"),
      "Yes",
      "No"
    ),
    TetanusVaccinated = as.factor(TetanusVaccinated)
  )
df <- df |> select(-TetanusLast10Tdap)
```

```{r tetanus plot pie}
plot_pie_freq(df$TetanusVaccinated, "Tetanus distribution")
```

Of the tetanus distribution we can see that it's an unbalanced class, having a `66.5%` of Yes and a `33.5%` of No.

#### Smoker

This class is an interesting one, we have decided to merge `ECigaretteUsage` and `SmokerStatus` as they convey the same semantic information of whether a person has smoked in his life. Firstly, we'll further discretize the values of each class:

```{r smoke_e_cig}
df <- df |>
  mutate(
    ECigaretteUsage = case_when(
      ECigaretteUsage == "Not at all (right now)" ~ "No",
      ECigaretteUsage == "Never used e-cigarettes in my entire life" ~ "No",
      ECigaretteUsage == "Use them every day" ~ "Yes",
      ECigaretteUsage == "Use them some days" ~ "Yes",
      TRUE ~ NA
    ),
    ECigaretteUsage = as.factor(ECigaretteUsage)
  )

df <- df |>
  mutate(
    SmokerStatus = case_when(
      SmokerStatus == "Never smoked" ~ "No",
      SmokerStatus == "Former smoker" ~ "Yes",
      SmokerStatus == "Current smoker - now smokes some days" ~ "Yes",
      SmokerStatus == "Current smoker - now smokes every day" ~ "Yes",
      TRUE ~ NA
    ),
    SmokerStatus = as.factor(SmokerStatus)
  )
```

The distribution of the `SmokerStatus` is the following:

```{r smoker pie plot}
plot_pie_freq(df$SmokerStatus, "Smoker distribution")
```

Even though the classes are unbalanced is not that a big deal compared to others, we have a `60.3%` of Yes and a `39.7%` of No.

The distribution of the `EcigaretteUsage` is the following:

```{r ecigarrete pie plot}
plot_pie_freq(df$ECigaretteUsage, "EcigaretteUsage distribution")
```

In this case we will need to do something with this class as it presents a high imbalance. As stated above, we plan on merging these two variables; such implementation can be found on further chapters when tackling variable fusion.

#### Race

In the case of `RaceEthnicityCategory`, we decided to include the NA's, about 10k observations, in the 'Multiracial' category. This decision aims to help balance the classes, as this category currently only has 8k entries.

```{r}
df <- df |>
  mutate(
    RaceEthnicityCategory = case_when(
      RaceEthnicityCategory == "Black only, Non-Hispanic" ~ "Black",
      RaceEthnicityCategory == "Multiracial, Non-Hispanic" ~ "Multiracial",
      RaceEthnicityCategory == "White only, Non-Hispanic" ~ "White",
      RaceEthnicityCategory == "Other race only, Non-Hispanic" ~ "Other",
      is.na(RaceEthnicityCategory) ~ "Multiracial",
      TRUE ~ RaceEthnicityCategory
    ),
    RaceEthnicityCategory = as.factor(RaceEthnicityCategory)
  )
```

The distribution by race is the following:

```{r race pie plot}
plot_pie_freq(df$RaceEthnicityCategory, "Race distribution")
```

As we can see there is the vast majority of people is White making this variable to maybe not be as good as others because of the imbalance.

#### Last check-up time

For `LastCheckupTime` we have decided to make our own factorization of the data, we have concluded that there is not a good or logical way to predict the NA's because it doesn't make sense to know if a person has gone to a check up using other variables. This took us to make the decision to make all the NA's part of the category of 5 or more years to try to balance a little bit the classes.

```{r last_check_time_map}
df <- df |>
  mutate(
    YearsSinceCheckup = case_when(
      LastCheckupTime == "Within past year (anytime less than 12 months ago)" ~ 1,
      LastCheckupTime == "Within past 2 years (1 year but less than 2 years ago)" ~ 2,
      LastCheckupTime == "Within past 5 years (2 years but less than 5 years ago)" ~ 5,
      LastCheckupTime == "5 or more years ago" ~ 6,
      is.na(LastCheckupTime) ~ 6,
      TRUE ~ 6
    )
  )

df <- df |> select(-LastCheckupTime)

df |> count(YearsSinceCheckup)
```

Here we can find the distribution of the `YearsSinceCheckup`:

```{r years_since_checkuo pie plot}
plot_pie_freq(df$YearsSinceCheckup, "YearsSinceCheckup distribution")
```

Even though we have tried to balance the classes we can see that there is a high unbalance of them.

#### Removed Teeth

For `RemovedTeeth` we have suposse that if we don't know if any teeth has been removed None of them have been removed.

```{r}
df <- df |>
  mutate(
    RemovedTeeth = case_when(
      is.na(RemovedTeeth) ~ "None of them",
      TRUE ~ RemovedTeeth
    ),
    RemovedTeeth = as.factor(RemovedTeeth)
  )

table(df$RemovedTeeth)
```

The distribution of `RemovedTeeth` is:

```{r RemovedTeeth pie plot}
plot_pie_freq(df$RemovedTeeth, "RemovedTeeth distribution")
```

As we can see if we take every class individually we can see the imbalance but if we look closer we can see that there is a `54.8%` of people that has not lost any teeth and a `45.2%` that has lost at least 1.

#### Sleep

Before seeing the distribution let's categorize this variable, taking in consideration the medical consensus found in an [official website ot the United States government](https://pubmed.ncbi.nlm.nih.gov/29073398/). We have decided to make all the NA's as "Optimal" because is the mean of hours that people normally sleeps.

```{r cat sleep}
df <- df %>%
  mutate(
    Sleep = case_when(
      SleepHours %in% 1:5 | SleepHours %in% 11:24 ~ "Not Appropriate",
      SleepHours %in% c(6, 10) ~ "Appropriate",
      SleepHours %in% 7:9 ~ "Optimal",
      is.na(SleepHours) ~ "Optimal",
    ),
    Sleep = as.factor(Sleep)
  )

df <- df |> select(-SleepHours)
```

Here we can see the distribution:

```{r sleep plot pie}
plot_pie_freq(df$Sleep, "Sleep distribution")
```

As we can see there is a `64.1%` of people that sleeps the reccomended amount of hours, on the other hand we have a `23.9%` of people that sleeps 6 or 10 hours, and a `11.9%` of people that sleeps less than 6 hours or more than 10.

#### Mental and Physical

Mental and Physical health days present values from 0-30. Preliminary observation suggests the existence of three distinct clusters within these variables. To objectively validate this intuition, we'll apply hierarchical clusterization. The next step involves the assignment of semantic labels:

- High
- Medium
- Low

With the intent of creating a new varialbe that will replace the original one, representing the index of health of the given replaced one.

As the clustering process cannot be done to the whole dataset, yielding an _infinite depth_ error, a sample of the data will be used instead. Consequently, as we want the whole data domain to be taken into account we won't be creating a fully random subset. We'll first insert one value for each class (0-30) and fill the remaining slots with true random samples.

```{r hclust mental data}
set.seed(29)

mhd_df <- data.frame(df$MentalHealthDays) |>
  na.omit() %>%
  slice_sample(n = 1170) # Remove 30 items that we will insert manually

mhd_df <- Reduce(
  function(x, y) merge(x, y, all = TRUE),
  list(mhd_df, tibble(df.MentalHealthDays = 0:30))
)

table(mhd_df)
```

With this we assure that at least we have one value for each day (as we'll be ignoring the NA values for obvious reasons)

```{r hclust mental}
mhd_labels <- make_hclust(
  mhd_df |> scale(),
  "MentalHealthDays Clustering Dendrogram"
) %>%
  tibble(Cluster_labels = .) %>%
  mutate(Cluster_labels = case_when(
    Cluster_labels == 1 ~ "High",
    Cluster_labels == 2 ~ "Medium",
    Cluster_labels == 3 ~ "Low"
  ))
```

```{r}
mhd_summary <- cbind(mhd_df, mhd_labels) |>
  group_by(Cluster_labels) |>
  summarise(
    MinValue = min(df.MentalHealthDays),
    MaxValue = max(df.MentalHealthDays)
  )
```

We'll create a new variable that discretizes the current numerical variable and remove the original numerical data

```{r}
df <- df |>
  mutate(
    MentalHealthIndex = case_when(
      MentalHealthDays %in% mhd_summary$MinValue[1]:mhd_summary$MaxValue[1] ~ mhd_summary$Cluster_labels[1],
      MentalHealthDays %in% mhd_summary$MinValue[2]:mhd_summary$MaxValue[2] ~ mhd_summary$Cluster_labels[2],
      MentalHealthDays %in% mhd_summary$MinValue[3]:mhd_summary$MaxValue[3] ~ mhd_summary$Cluster_labels[3],
    ),
    MentalHealthIndex = as.factor(MentalHealthIndex)
  )

df <- df |> select(-MentalHealthDays)
```

We will apply the same as we did for the mental health days, but this time we will use the `PhysicalHealthDays` variable.

```{r hclust physical}
set.seed(29)

# Create a subset of data, ignoring NA and taking a sample of 1200 observations
phd_df <- data.frame(df$PhysicalHealthDays) |>
  na.omit() %>%
  slice_sample(n = 1170) # Remove 30 items that we will insert manually

phd_df <- Reduce(
  function(x, y) merge(x, y, all = TRUE),
  list(phd_df, tibble(df.PhysicalHealthDays = 0:30))
)

# Clusterize the data and assign semantic labels for each one of them
phd_labels <- make_hclust(
  phd_df |> scale(),
  "PhysicalHealthDays Clustering Dendrogram"
) %>%
  tibble(Cluster_labels = .) %>%
  mutate(Cluster_labels = case_when(
    Cluster_labels == 1 ~ "High",
    Cluster_labels == 2 ~ "Medium",
    Cluster_labels == 3 ~ "Low"
  ))

# Create a range summary for each of the semantic labels
phd_summary <- cbind(phd_df, phd_labels) |>
  group_by(Cluster_labels) |>
  summarise(
    MinValue = min(df.PhysicalHealthDays),
    MaxValue = max(df.PhysicalHealthDays)
  )

# Create a new variable that discretizes the current numerical variable
df <- df |>
  mutate(
    PhysicalHealthIndex = case_when(
      PhysicalHealthDays %in% phd_summary$MinValue[1]:phd_summary$MaxValue[1] ~ phd_summary$Cluster_labels[1],
      PhysicalHealthDays %in% phd_summary$MinValue[2]:phd_summary$MaxValue[2] ~ phd_summary$Cluster_labels[2],
      PhysicalHealthDays %in% phd_summary$MinValue[3]:phd_summary$MaxValue[3] ~ phd_summary$Cluster_labels[3],
    ),
    PhysicalHealthIndex = as.factor(PhysicalHealthIndex)
  )

# Remove the original numerical data
df <- df |> select(-PhysicalHealthDays)
```

### Data generation

### Predictive models

To conclude with the missing values, we will attempt to predict the features with NA's using observations without any missing values, which constitute 61% of the data. The variables we are trying to predict are binary, meaning they have values of 'Yes' or 'No.' We will use a decision tree, we will substitute all the values predicted but, we have to take into consideration that there are some that doesn't exceed our threshold, i.e., accuracy > 0.8.

```{r sandbox predictive_models}
non_na <- df[rowSums(is.na(df)) == 0, ]
na_rows <- df[rowSums(is.na(df)) > 0, ]
na_rows_mutable <- df[rowSums(is.na(df)) > 0, ]

data_split <- initial_split(non_na, prop = 0.8)

bool_cols <- df %>%
  select(where(~ all(. %in% c("Yes", "No", NA)))) |>
  select(-HadHeartAttack) |>
  colnames()

model <- decision_tree(
  engine = "rpart",
  mode = "classification"
)

for (name in bool_cols) {
  cat("[DOING  ]", name, "\n")
  model_output <- model_accuracy(
    model,
    training(data_split),
    testing(data_split),
    name
  )

  if (model_output$metrics$accuracy < 0.8) {
    cat("[WARNING]", name, "had a subpar accuracy of", model_output$metrics$accuracy, "\n")
  }

  cat("[REPLACE]", name, "is a good candidate, having an accuracy of", model_output$metrics$accuracy, "\n")

  pred <- model_output$model |>
    predict(na_rows) |>
    pull(.pred_class)

  # Replace only the missing values in na_rows for the given column
  for (value in seq_along(na_rows_mutable[[name]])) {
    if (is.na(na_rows_mutable[[name]][value])) {
      na_rows_mutable[[name]][value] <- pred[value]
    }
  }
}
```

The predictions that has not exceed the threshold are `PhysicalActivities`, `HadArthritis`, `SmokerStatus`, `ChestScan`, `AlcoholDrinkers`, `HIVTesting`, `FluVaxLast12`, `TetanusVaccinated` we will take those predictions to get rid of the NA's in a meaningfull way.

We merge the rows with non-NA and those rows that the NA value has been predicted.

```{r merge df}
df <- Reduce(
  function(x, y) merge(x, y, all = TRUE),
  list(non_na, na_rows_mutable)
)
```

### Recodification

We'll map the "Yes/No" values to 1/0 respectively to make it easier to work with them

```{r boolean_map}
bool_cols <- df |>
  select(where(~ any(. %in% c("Yes", "No")))) |>
  colnames()

for (col in bool_cols) {
  df[[col]] <- ifelse(df[[col]] == "No", 0, 1)
}
```

### Fusion of classes

#### Smoking

Due to the small quantity of values, approximately 3000 'Not Knowns,' we will add these values to the class with fewer data points, in this case, 'Yes,' as illustrated in the pie chart below.

```{r smoking_map}
df <- df |>
  mutate(
    HasSmoked = case_when(
      SmokerStatus == 0 | ECigaretteUsage == 0 ~ 0,
      is.na(SmokerStatus) & is.na(ECigaretteUsage) ~ sample(c(1, 0), 1, replace = TRUE),
      is.na(SmokerStatus) ~ ECigaretteUsage,
      is.na(ECigaretteUsage) ~ SmokerStatus,
      TRUE ~ 1
    )
  )

df <- df |> select(-SmokerStatus, -ECigaretteUsage)
```

Here we have the distribution:

```{r smoked pie plot}
plot_pie_freq(df$HasSmoked, "HasSmoked distribution")
```

#### Difficulties

Now we'll try to explore the correlation between the difficulties to see if we can merge them into a single variable

```{r}
corr_df <- na.omit(df)
ggcorr(
  corr_df %>% select(contains("Difficulty")),
  label = TRUE,
  label_round = 2,
  hjust = 0.75,
  angle = -45
)
```

As we can see, the difficulties are somewhat correlated, so we'll create a new variable `LifeDifficulties` by adding them to account for multiple difficulties at the same time.

```{r}
df <- df %>%
  mutate(
    LifeDifficulties = rowSums(select(., contains("Difficulty"))),
    across(contains("Difficulty"), ~NULL)
  )
```

### WeightInKilograms and HeightInMeters treatment

Now we can use all the data in the dataset to predict the height and the weight.

```{r data_gen weight}
weight_model <- rpart(WeightInKilograms ~ ., data = df, method = "anova")
df$WeightInKilograms[is.na(df$WeightInKilograms)] <- predict(weight_model, newdata = df[is.na(df$WeightInKilograms), ])
model_statistics(num_samples = 10000, target_variable = "WeightInKilograms", dataframe = df)
```

We can see a `84%` of accuracy when predicting Weight.

```{r data_gen height}
height_model <- rpart(HeightInMeters ~ ., data = df, method = "anova")
df$HeightInMeters[is.na(df$HeightInMeters)] <- predict(height_model, newdata = df[is.na(df$HeightInMeters), ])
model_statistics(num_samples = 10000, target_variable = "HeightInMeters", dataframe = df)
```

We can see a `67.5%` of accuracy when predicting Height.

After predicting the values of `WeightInKilograms` and `HeightInMeters` we can calculate the BMI of those values that are NA using it's formula:

$$BMI = \frac{Weight}{Height^2}$$

```{r}
df$BMI <- df$WeightInKilograms / (df$HeightInMeters^2)

df <- df |> select(-WeightInKilograms, -HeightInMeters)
```

```{r sandbox BMI}
BMI_subset <- subset(df, select = c("State", "BMI", "HadHeartAttack"))
# DEBUG
df |>
  select(where(~ any(is.na(.)))) |>
  mutate(across(where(is.character), as.factor)) |>
  summary()
```

After the calculation we can remove `WeightInKilograms` and `HeightInMeters` because they are redundant once we have BMI.

#### BMI

Exploring the BMI distribution

```{r}
boxplot(df$BMI)
```

Seeing the distribution of the BMI, we can see that there are some outliers, to deal with them we'll group them into categories based on the CDC's BMI classification. See [this](https://www.cdc.gov/obesity/basics/adult-defining.html#:~:text=If%20your%20BMI%20is%20less,falls%20within%20the%20obesity%20range) link: The categories are:

- Underweight: BMI is less than 18.5
- Healthy weight: BMI is 18.5 to 24.9
- Overweight: BMI is 25 to 29.9
- Obese: BMI of 30 or greater
  - Obese Class 1: BMI of 30 to 34.9
  - Obese Class 2: BMI of 35 to 39.9
  - Obese Class 3: BMI of 40 or greater

```{r BMI_bin}
bmi_categories <- c("Underweight", "Healthy Weight", "Overweight", "Obese C1", "Obese C2", "Obese C3")
bmi_ranges <- c(0, 18.5, 24.9, 29.9, 34.9, 39.9, Inf)

df$BMI <- cut(
  df$BMI,
  breaks = bmi_ranges,
  labels = bmi_categories,
  include.lowest = TRUE
)
```

Once we have the categories, we can plot the distribution of the BMI

```{r BMI pie plot}
plot_pie_freq(df$BMI, "BMI distribution")
```

Observing the distribution of the BMI, we can see that the majority of the people are in the overweight category, followed by the obese category. The health and underweight categories are the least populated.

### Duplicated observations

Once all the data is cleaned and all the variables are in the correct format, we can proceed to remove the duplicated observations. This is done to avoid having the same observation in both the training and testing sets, which would lead to overfitting.

We are removing all the duplicated observations, after applying the feature engineering and whatnot, to avoid having the same observation while we were cleaning the data.

```{r distinct_removal}
cat("Removing", sum(duplicated(df)), "duplicated observations")
df <- unique(df)
```

Finally, we can see the structure of the dataframe cleaned and ready to be used for analysis and the creation of models.

```{r}
str(df)
```

### NA check

```{r sandbox2}
df %>%
  summarise_all(~ sum(is.na(.))) %>%
  select_if(~ . > 0)
```

Seeing this amount of NA's we have concluded that the best way to take care of them is to add them to the class with least amount of observations for each variable with the objective of slightly balance the classes. As `HadHeartAttack` is our target, we won't be replacing any NA values, instead, this subset will be used as input to our final predictive function.

```{r}
df <- df %>%
  mutate(across(-HadHeartAttack, ~ ifelse(is.na(.), which.min(table(.)), .)))
```

```{r sandbox3}
df %>%
  summarise_all(~ sum(is.na(.))) %>%
  select_if(~ . > 0)
```

### Export cleaned dataframe to csv

Save the cleaned dataframe to a csv file to be used in the `models.Rmd` file.

```{r}
str(df)
```

```{r export dataframe}
write.csv(df, here("data", "clean.csv"))
```

## Data visualization

```{r corr_matrix}
ggcorr(
  df,
  label = TRUE,
  label_round = 2,
  hjust = 0.75,
  angle = -45
)
```

!TODO Explain some patters of the correlation matrix that seem interesting for our paper

```{r}
write.csv(BMI_subset, here("data", "BMI.csv"))
```

```{r BMI_heart_attack_state}
BMI_subset <- read.csv(here("data", "BMI.csv"))

state_abbreviations <- c("DC", "GU", "PR", "VI")

str(BMI_subset)
BMI_state <- BMI_subset |>
  mutate(State = as.character(State)) |>
  group_by(State) |>
  summarize(
    mean_BMI = mean(BMI, na.rm = TRUE),
    HeartAttacks = sum(HadHeartAttack, na.rm = TRUE)
  ) |>
  arrange(State) |>
  mutate(StateAbb = state.abb[match(State, state.name)]) |>
  mutate(StateAbb = case_when(
    State == "District of Columbia" ~ "DC",
    State == "Guam" ~ "GU",
    State == "Puerto Rico" ~ "PR",
    State == "Virgin Islands" ~ "VI",
    TRUE ~ StateAbb
  ))

BMI_state$lab <- paste(BMI_state$StateAbb, round(BMI_state$mean_BMI, 2), sep = "\n")

BMI_state <- BMI_state |>
  filter(State != "Guam")

sb <- statebins(
  state_data = BMI_state,
  state_col = "State",
  value_col = "HeartAttacks"
) +
  labs(title = "Heart attack distribution and BMI mean per state") +
  theme_void() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.line = element_blank(),
    plot.title = element_text(hjust = 0.5),
    plot.background = element_blank(),
    strip.background = element_blank()
  )

max_bmi_state <- BMI_state[which.max(BMI_state$mean_BMI), "StateAbb"]
min_bmi_state <- BMI_state[which.min(BMI_state$mean_BMI), "StateAbb"]

# https://stackoverflow.com/questions/76574854/how-to-add-count-labels-to-statebin-map
sb <- ggplot_build(sb)
sb$data[[2]]$colour[sb$data[[2]]$label %in% max_bmi_state] <- "red"
sb$data[[2]]$colour[sb$data[[2]]$label %in% min_bmi_state] <- "green"
sb$data[[2]]$label <- BMI_state$lab[match(sb$data[[2]]$label, BMI_state$StateAbb)]
sb <- ggplot_gtable(sb)

plot(sb)
```

```{r states_map_presentation, warning=FALSE}
state_counts <- table(df$State)

# Merge the counts with the original data
df_with_counts <- merge(
  df,
  data.frame(
    State = names(state_counts),
    Count = as.numeric(state_counts)
  ),
  by = "State",
  all.x = TRUE
)

# Plot the choropleth map using statebins_continuous
statebins(
  state_data = df_with_counts |> mutate(State = as.character(State)),
  state_col = "State",
  value_col = "Count"
) +
  labs(title = "Number of Observations in Each State") +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.line = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```
